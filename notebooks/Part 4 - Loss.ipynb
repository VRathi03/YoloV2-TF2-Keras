{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.9 (default, Aug 31 2020, 12:42:55) \n",
      "[GCC 7.3.0]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "print(sys.version)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANCHORS = np.array([1.07709888,  1.78171903,  # anchor box 1, width , height\n",
    "                    2.71054693,  5.12469308,  # anchor box 2, width,  height\n",
    "                   10.47181473, 10.09646365,  # anchor box 3, width,  height\n",
    "                    5.48531347,  8.11011331]) # anchor box 4, width,  height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = ['aeroplane',  'bicycle', 'bird',  'boat',      'bottle', \n",
    "          'bus',        'car',      'cat',  'chair',     'cow',\n",
    "          'diningtable','dog',    'horse',  'motorbike', 'person',\n",
    "          'pottedplant','sheep',  'sofa',   'train',   'tvmonitor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N train = 22263\n"
     ]
    }
   ],
   "source": [
    "train_image_folder = \"/media/vkr/VKR/WorkSpace/Deep Learning Projects/yolov2/data/processed/VOC2012/JPEGImages/\"\n",
    "train_annot_folder = \"/media/vkr/VKR/WorkSpace/Deep Learning Projects/yolov2/data/processed/VOC2012/Annotations/\"\n",
    "\n",
    "np.random.seed(1)\n",
    "from backend import parse_annotation\n",
    "train_image, seen_train_labels = parse_annotation(train_annot_folder,\n",
    "                                                  train_image_folder, \n",
    "                                                  labels=LABELS)\n",
    "print(\"N train = {}\".format(len(train_image)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from backend import SimpleBatchGenerator\n",
    "\n",
    "BATCH_SIZE        = 2\n",
    "IMAGE_H, IMAGE_W  = 416, 416\n",
    "GRID_H,  GRID_W   = 13 , 13\n",
    "TRUE_BOX_BUFFER   = 50\n",
    "BOX               = int(len(ANCHORS)/2)\n",
    "# CLASS             = len(LABELS)\n",
    "\n",
    "generator_config = {\n",
    "    'IMAGE_H'         : IMAGE_H, \n",
    "    'IMAGE_W'         : IMAGE_W,\n",
    "    'GRID_H'          : GRID_H,  \n",
    "    'GRID_W'          : GRID_W,\n",
    "    'LABELS'          : LABELS,\n",
    "    'ANCHORS'         : ANCHORS,\n",
    "    'BATCH_SIZE'      : BATCH_SIZE,\n",
    "    'TRUE_BOX_BUFFER' : TRUE_BOX_BUFFER,\n",
    "}\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    return image / 255.\n",
    "train_batch_generator = SimpleBatchGenerator(train_image, generator_config,\n",
    "                                             norm=normalize, shuffle=True)\n",
    "\n",
    "[x_batch,b_batch],y_batch = train_batch_generator.__getitem__(idx=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA_NO_OBJECT = 1.0\n",
    "LAMBDA_OBJECT    = 5.0\n",
    "LAMBDA_COORD     = 1.0\n",
    "LAMBDA_CLASS     = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cell_grid(GRID_W,GRID_H,BATCH_SIZE,BOX): \n",
    "    '''\n",
    "    Helper function to assure that the bounding box x and y are in the grid cell scale\n",
    "    == output == \n",
    "    for any i=0,1..,batch size - 1\n",
    "    output[i,5,3,:,:] = array([[3., 5.],\n",
    "                               [3., 5.],\n",
    "                               [3., 5.]], dtype=float32)\n",
    "    '''\n",
    "    ## cell_x.shape = (1, 13, 13, 1, 1)\n",
    "    ## cell_x[:,i,j,:] = [[[j]]]\n",
    "    cell_x = tf.cast(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)), tf.float32)\n",
    "    # tf.cast(num, tf.float32)\n",
    "    ## cell_y.shape = (1, 13, 13, 1, 1)\n",
    "    ## cell_y[:,i,j,:] = [[[i]]]\n",
    "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "    ## cell_gird.shape = (16, 13, 13, 5, 2)\n",
    "    ## for any n, k, i, j\n",
    "    ##    cell_grid[n, i, j, anchor, k] = j when k = 0\n",
    "    ## for any n, k, i, j\n",
    "    ##    cell_grid[n, i, j, anchor, k] = i when k = 1    \n",
    "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, BOX, 1])\n",
    "    return(cell_grid) \n",
    "\n",
    "def adjust_scale_prediction(y_pred, cell_grid, ANCHORS):    \n",
    "    \"\"\"\n",
    "        Adjust prediction\n",
    "        \n",
    "        == input ==\n",
    "        \n",
    "        y_pred : takes any real values\n",
    "                 tensor of shape = (N batch, NGrid h, NGrid w, NAnchor, 4 + 1 + N class)\n",
    "        \n",
    "        ANCHORS : list containing width and height specializaiton of anchor box\n",
    "        == output ==\n",
    "        \n",
    "        pred_box_xy : shape = (N batch, N grid x, N grid y, N anchor, 2), contianing [center_y, center_x] rangining [0,0]x[grid_H-1,grid_W-1]\n",
    "          pred_box_xy[irow,igrid_h,igrid_w,ianchor,0] =  center_x\n",
    "          pred_box_xy[irow,igrid_h,igrid_w,ianchor,1] =  center_1\n",
    "          \n",
    "          calculation process:\n",
    "          tf.sigmoid(y_pred[...,:2]) : takes values between 0 and 1\n",
    "          tf.sigmoid(y_pred[...,:2]) + cell_grid : takes values between 0 and grid_W - 1 for x coordinate \n",
    "                                                   takes values between 0 and grid_H - 1 for y coordinate \n",
    "                                                   \n",
    "        pred_Box_wh : shape = (N batch, N grid h, N grid w, N anchor, 2), containing width and height, rangining [0,0]x[grid_H-1,grid_W-1]\n",
    "        \n",
    "        pred_box_conf : shape = (N batch, N grid h, N grid w, N anchor, 1), containing confidence to range between 0 and 1\n",
    "        \n",
    "        pred_box_class : shape = (N batch, N grid h, N grid w, N anchor, N class), containing \n",
    "    \"\"\"\n",
    "    BOX = int(len(ANCHORS)/2)\n",
    "    ## cell_grid is of the shape of \n",
    "    \n",
    "    ### adjust x and y  \n",
    "    # the bounding box bx and by are rescaled to range between 0 and 1 for given gird.\n",
    "    # Since there are BOX x BOX grids, we rescale each bx and by to range between 0 to BOX + 1\n",
    "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid # bx, by\n",
    "    \n",
    "    ### adjust w and h\n",
    "    # exp to make width and height positive\n",
    "    # rescale each grid to make some anchor \"good\" at representing certain shape of bounding box \n",
    "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS,[1,1,1,BOX,2]) # bw, bh\n",
    "\n",
    "    ### adjust confidence \n",
    "    pred_box_conf = tf.sigmoid(y_pred[..., 4])# prob bb\n",
    "\n",
    "    ### adjust class probabilities \n",
    "    pred_box_class = y_pred[..., 5:] # prC1, prC2, ..., prC20\n",
    "    \n",
    "    return(pred_box_xy,pred_box_wh,pred_box_conf,pred_box_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "prepare inputs\n",
      "y_pred before scaling = (2, 13, 13, 4, 25)\n",
      "******************************\n",
      "define tensor graph\n",
      "******************************\n",
      "ouput\n",
      "******************************\n",
      "\n",
      "pred_box_xy (2, 13, 13, 4, 2)\n",
      "  bounding box x at iGRID_W=00 MIN= 0.47, MAX= 0.53\n",
      "  bounding box x at iGRID_W=01 MIN= 1.47, MAX= 1.53\n",
      "  bounding box x at iGRID_W=02 MIN= 2.46, MAX= 2.53\n",
      "  bounding box x at iGRID_W=03 MIN= 3.45, MAX= 3.53\n",
      "  bounding box x at iGRID_W=04 MIN= 4.47, MAX= 4.54\n",
      "  bounding box x at iGRID_W=05 MIN= 5.47, MAX= 5.53\n",
      "  bounding box x at iGRID_W=06 MIN= 6.46, MAX= 6.53\n",
      "  bounding box x at iGRID_W=07 MIN= 7.47, MAX= 7.55\n",
      "  bounding box x at iGRID_W=08 MIN= 8.47, MAX= 8.53\n",
      "  bounding box x at iGRID_W=09 MIN= 9.46, MAX= 9.53\n",
      "  bounding box x at iGRID_W=10 MIN=10.46, MAX=10.53\n",
      "  bounding box x at iGRID_W=11 MIN=11.46, MAX=11.54\n",
      "  bounding box x at iGRID_W=12 MIN=12.46, MAX=12.54\n",
      "  bounding box y at iGRID_H=00 MIN= 0.47, MAX= 0.53\n",
      "  bounding box y at iGRID_H=01 MIN= 1.46, MAX= 1.53\n",
      "  bounding box y at iGRID_H=02 MIN= 2.47, MAX= 2.56\n",
      "  bounding box y at iGRID_H=03 MIN= 3.46, MAX= 3.54\n",
      "  bounding box y at iGRID_H=04 MIN= 4.46, MAX= 4.53\n",
      "  bounding box y at iGRID_H=05 MIN= 5.46, MAX= 5.55\n",
      "  bounding box y at iGRID_H=06 MIN= 6.47, MAX= 6.54\n",
      "  bounding box y at iGRID_H=07 MIN= 7.47, MAX= 7.54\n",
      "  bounding box y at iGRID_H=08 MIN= 8.45, MAX= 8.54\n",
      "  bounding box y at iGRID_H=09 MIN= 9.46, MAX= 9.53\n",
      "  bounding box y at iGRID_H=10 MIN=10.47, MAX=10.53\n",
      "  bounding box y at iGRID_H=11 MIN=11.47, MAX=11.54\n",
      "  bounding box y at iGRID_H=12 MIN=12.47, MAX=12.53\n",
      "\n",
      "pred_box_wh (2, 13, 13, 4, 2)\n",
      "  bounding box width  MIN= 0.90, MAX=12.23\n",
      "  bounding box height MIN= 1.54, MAX=11.89\n",
      "\n",
      "pred_box_conf (2, 13, 13, 4)\n",
      "  confidence  MIN= 0.45, MAX= 0.56\n",
      "\n",
      "pred_box_class (2, 13, 13, 4, 20)\n",
      "  class probability MIN=-0.23, MAX= 0.26\n"
     ]
    }
   ],
   "source": [
    "def print_min_max(vec,title):\n",
    "    try:\n",
    "        print(\"{} MIN={:5.2f}, MAX={:5.2f}\".format(\n",
    "            title, np.min(vec),np.max(vec)))\n",
    "    except ValueError:  #raised if `y` is empty.\n",
    "        pass\n",
    "\n",
    "print(\"*\"*30)\n",
    "print(\"prepare inputs\")\n",
    "GRID_W = 13 \n",
    "GRID_H = 13 \n",
    "BOX    = int(len(ANCHORS)/2)\n",
    "CLASS = len(LABELS)\n",
    "size   = BATCH_SIZE*GRID_W*GRID_H*BOX*(4 + 1 + CLASS)\n",
    "y_pred = np.random.normal(size=size,scale = 10/(GRID_H*GRID_W)) \n",
    "y_pred = y_pred.reshape(BATCH_SIZE,GRID_H,GRID_W,BOX,4 + 1 + CLASS)\n",
    "print(\"y_pred before scaling = {}\".format(y_pred.shape))\n",
    "\n",
    "print(\"*\"*30)\n",
    "print(\"define tensor graph\")\n",
    "y_pred_tf = tf.constant(y_pred,dtype=\"float32\")\n",
    "cell_grid = get_cell_grid(GRID_W,GRID_H,BATCH_SIZE,BOX)\n",
    "\n",
    "(pred_box_xy,   pred_box_wh, \n",
    " pred_box_conf, pred_box_class) = adjust_scale_prediction(y_pred_tf, \n",
    "                                                                cell_grid, \n",
    "                                                                ANCHORS)\n",
    "print(\"*\"*30 + \"\\nouput\\n\" + \"*\"*30) \n",
    "# with tf.Session() as sess:\n",
    "#     (pred_box_xy, pred_box_wh, pred_box_conf, pred_box_class) = sess.run([pred_box_xy_tf,   pred_box_wh_tf, pred_box_conf_tf, pred_box_class_tf])\n",
    "    \n",
    "print(\"\\npred_box_xy {}\".format(pred_box_xy.shape))           \n",
    "\n",
    "for igrid_w in range(pred_box_xy.shape[2]):\n",
    "        print_min_max(pred_box_xy[:,:,igrid_w,:,0],\n",
    "                      \"  bounding box x at iGRID_W={:02.0f}\".format(igrid_w))\n",
    "for igrid_h in range(pred_box_xy.shape[1]):\n",
    "    print_min_max(pred_box_xy[:,igrid_h,:,:,1],\n",
    "                  \"  bounding box y at iGRID_H={:02.0f}\".format(igrid_h)) \n",
    "    \n",
    "print(\"\\npred_box_wh {}\".format(pred_box_wh.shape))\n",
    "print_min_max(pred_box_wh[:,:,:,:,0],\"  bounding box width \") \n",
    "print_min_max(pred_box_wh[:,:,:,:,1],\"  bounding box height\") \n",
    "    \n",
    "print(\"\\npred_box_conf {}\".format(pred_box_conf.shape))\n",
    "print_min_max(pred_box_conf,\"  confidence \") \n",
    "\n",
    "print(\"\\npred_box_class {}\".format(pred_box_class.shape))\n",
    "print_min_max(pred_box_class,\"  class probability\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Extract ground truth output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ground_truth(y_true):    \n",
    "    true_box_xy    = y_true[..., 0:2] # bounding box x, y coordinate in grid cell scale \n",
    "    true_box_wh    = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
    "    true_box_conf  = y_true[...,4]    # confidence \n",
    "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
    "    return(true_box_xy, true_box_wh, true_box_conf, true_box_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input y_batch = (2, 13, 13, 4, 25)\n",
      "******************************\n",
      "ouput\n",
      "******************************\n",
      "\n",
      "true_box_xy (2, 13, 13, 4, 2)\n",
      "  bounding box x at iGRID_W=00 MIN= 0.36, MAX= 0.36\n",
      "  bounding box x at iGRID_W=04 MIN= 4.14, MAX= 4.22\n",
      "  bounding box x at iGRID_W=05 MIN= 5.20, MAX= 5.81\n",
      "  bounding box x at iGRID_W=06 MIN= 6.89, MAX= 6.89\n",
      "  bounding box x at iGRID_W=07 MIN= 7.16, MAX= 7.48\n",
      "  bounding box x at iGRID_W=09 MIN= 9.84, MAX= 9.84\n",
      "  bounding box x at iGRID_W=11 MIN=11.50, MAX=11.50\n",
      "  bounding box y at iGRID_H=05 MIN= 5.34, MAX= 5.36\n",
      "  bounding box y at iGRID_H=06 MIN= 6.05, MAX= 6.05\n",
      "  bounding box y at iGRID_H=07 MIN= 7.02, MAX= 7.41\n",
      "  bounding box y at iGRID_H=08 MIN= 8.70, MAX= 8.70\n",
      "  bounding box y at iGRID_H=10 MIN=10.05, MAX=10.05\n",
      "  bounding box y at iGRID_H=11 MIN=11.31, MAX=11.31\n",
      "  bounding box y at iGRID_H=12 MIN=12.20, MAX=12.20\n",
      "\n",
      "true_box_wh (2, 13, 13, 4, 2)\n",
      "  bounding box width  MIN= 0.00, MAX=10.41\n",
      "  bounding box height MIN= 0.00, MAX=11.47\n",
      "\n",
      "true_box_conf (2, 13, 13, 4)\n",
      "  confidence, unique value = [0. 1.]\n",
      "\n",
      "true_box_class (2, 13, 13, 4)\n",
      "  class index, unique value = [ 0 11 16]\n"
     ]
    }
   ],
   "source": [
    "# y_batch is the output of the simpleBatchGenerator.fit()\n",
    "print(\"Input y_batch = {}\".format(y_batch.shape))\n",
    "\n",
    "y_batch_tf = tf.constant(y_batch,dtype=\"float32\")\n",
    "(true_box_xy, true_box_wh, \n",
    " true_box_conf, true_box_class) = extract_ground_truth(y_batch_tf)\n",
    "\n",
    "print(\"*\"*30 + \"\\nouput\\n\" + \"*\"*30) \n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#         (true_box_xy, true_box_wh, \n",
    "#          true_box_conf, true_box_class) = sess.run(\n",
    "#                     [true_box_xy_tf,   true_box_wh_tf, \n",
    "#                      true_box_conf_tf, true_box_class_tf])\n",
    "\n",
    "print(\"\\ntrue_box_xy {}\".format(true_box_xy.shape))        \n",
    "for igrid_w in range(true_box_xy.shape[2]):\n",
    "    vec  = true_box_xy[:,:,igrid_w,:,0]\n",
    "    pick = true_box_conf[:,:,igrid_w,:] == 1 ## only pick C_ij = 1\n",
    "    print_min_max(vec[pick],\"  bounding box x at iGRID_W={:02.0f}\".format(igrid_w))\n",
    "    \n",
    "for igrid_h in range(true_box_xy.shape[1]):\n",
    "    vec  = true_box_xy[:,igrid_h,:,:,1]\n",
    "    pick = true_box_conf[:,igrid_h,:,:] == 1 ## only pick C_ij = 1\n",
    "    print_min_max(vec[pick],\"  bounding box y at iGRID_H={:02.0f}\".format(igrid_h)) \n",
    "    \n",
    "print(\"\\ntrue_box_wh {}\".format(true_box_wh.shape))\n",
    "print_min_max(true_box_wh[:,:,:,:,0],\"  bounding box width \") \n",
    "print_min_max(true_box_wh[:,:,:,:,1],\"  bounding box height\") \n",
    "    \n",
    "print(\"\\ntrue_box_conf {}\".format(true_box_conf.shape))\n",
    "print(\"  confidence, unique value = {}\".format(np.unique(true_box_conf))) \n",
    "\n",
    "print(\"\\ntrue_box_class {}\".format(true_box_class.shape))\n",
    "print(\"  class index, unique value = {}\".format(np.unique(true_box_class)) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: calculate lossxywhi,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_xywh(true_box_conf,\n",
    "                   COORD_SCALE,\n",
    "                   true_box_xy, pred_box_xy,true_box_wh,pred_box_wh):  \n",
    "    '''\n",
    "    coord_mask:      np.array of shape (Nbatch, Ngrid h, N grid w, N anchor, 1)\n",
    "                     lambda_{coord} L_{i,j}^{obj}     \n",
    "                         \n",
    "    '''\n",
    "    \n",
    "    # lambda_{coord} L_{i,j}^{obj} \n",
    "    # np.array of shape (Nbatch, Ngrid h, N grid w, N anchor, 1)\n",
    "    coord_mask  = tf.expand_dims(true_box_conf, axis=-1) * COORD_SCALE \n",
    "    nb_coord_box = tf.reduce_sum(tf.cast(coord_mask > 0.0, tf.float32))\n",
    "    \n",
    "    loss_xy      = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy) * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    loss_wh      = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh) * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
    "    return(loss_xy + loss_wh, coord_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "ouput\n",
      "******************************\n",
      "loss_xywh = 2.542\n"
     ]
    }
   ],
   "source": [
    "# Experiment calc_loss_xywh\n",
    "\n",
    "LAMBDA_COORD = 1\n",
    "loss_xywh, coord_mask  = calc_loss_xywh(true_box_conf,LAMBDA_COORD,\n",
    "                                               true_box_xy, pred_box_xy,true_box_wh,pred_box_wh)\n",
    "\n",
    "print(\"*\"*30 + \"\\nouput\\n\" + \"*\"*30) \n",
    "\n",
    "# with tf.Session() as sess:\n",
    "#         loss_xywh = sess.run([loss_xywh_tf])[0]\n",
    "print(\"loss_xywh = {:4.3f}\".format(loss_xywh))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: calculate losspi,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_class(true_box_conf,CLASS_SCALE, true_box_class,pred_box_class):\n",
    "    '''\n",
    "    == input ==    \n",
    "    true_box_conf  : tensor of shape (N batch, N grid h, N grid w, N anchor)\n",
    "    true_box_class : tensor of shape (N batch, N grid h, N grid w, N anchor), containing class index\n",
    "    pred_box_class : tensor of shape (N batch, N grid h, N grid w, N anchor, N class)\n",
    "    CLASS_SCALE    : 1.0\n",
    "    \n",
    "    == output ==  \n",
    "    class_mask\n",
    "    if object exists in this (grid_cell, anchor) pair and the class object receive nonzero weight\n",
    "        class_mask[iframe,igridy,igridx,ianchor] = 1 \n",
    "    else: \n",
    "        0 \n",
    "    '''   \n",
    "    class_mask   = true_box_conf  * CLASS_SCALE ## L_{i,j}^obj * lambda_class\n",
    "    \n",
    "    nb_class_box = tf.reduce_sum(tf.cast(class_mask > 0.0, tf.float32))\n",
    "    loss_class   = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = true_box_class, \n",
    "                                                                  logits = pred_box_class)\n",
    "    loss_class   = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)   \n",
    "    return loss_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "ouput\n",
      "******************************\n",
      "loss_class = 2.997\n"
     ]
    }
   ],
   "source": [
    "LAMBDA_CLASS   = 1\n",
    "loss_class  = calc_loss_class(true_box_conf,LAMBDA_CLASS,\n",
    "                                 true_box_class,pred_box_class)\n",
    "print(\"*\"*30 + \"\\nouput\\n\" + \"*\"*30) \n",
    "print(\"loss_class = {:4.3f}\".format(loss_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5, calculate IOUground truthi,j ; preduictioni,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersect_area(true_xy,true_wh,\n",
    "                       pred_xy,pred_wh):\n",
    "    '''\n",
    "    == INPUT ==\n",
    "    true_xy,pred_xy, true_wh and pred_wh must have the same shape length\n",
    "\n",
    "    p1 : pred_mins = (px1,py1)\n",
    "    p2 : pred_maxs = (px2,py2)\n",
    "    t1 : true_mins = (tx1,ty1) \n",
    "    t2 : true_maxs = (tx2,ty2) \n",
    "                 p1______________________ \n",
    "                 |      t1___________   |\n",
    "                 |       |           |  |\n",
    "                 |_______|___________|__|p2 \n",
    "                         |           |rmax\n",
    "                         |___________|\n",
    "                                      t2\n",
    "    intersect_mins : rmin = t1  = (tx1,ty1)\n",
    "    intersect_maxs : rmax = (rmaxx,rmaxy)\n",
    "    intersect_wh   : (rmaxx - tx1, rmaxy - ty1)\n",
    "        \n",
    "    '''\n",
    "    true_wh_half = true_wh / 2.\n",
    "    true_mins    = true_xy - true_wh_half\n",
    "    true_maxes   = true_xy + true_wh_half\n",
    "    \n",
    "    pred_wh_half = pred_wh / 2.\n",
    "    pred_mins    = pred_xy - pred_wh_half\n",
    "    pred_maxes   = pred_xy + pred_wh_half    \n",
    "    \n",
    "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "    \n",
    "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "    union_areas = pred_areas + true_areas - intersect_areas\n",
    "    iou_scores  = tf.truediv(intersect_areas, union_areas)    \n",
    "    return iou_scores\n",
    "\n",
    "def calc_IOU_pred_true_assigned(true_box_conf,\n",
    "                                true_box_xy, true_box_wh,\n",
    "                                pred_box_xy,  pred_box_wh):\n",
    "    ''' \n",
    "    == input ==\n",
    "    \n",
    "    true_box_conf : tensor of shape (N batch, N grid h, N grid w, N anchor )\n",
    "    true_box_xy   : tensor of shape (N batch, N grid h, N grid w, N anchor , 2)\n",
    "    true_box_wh   : tensor of shape (N batch, N grid h, N grid w, N anchor , 2)\n",
    "    pred_box_xy   : tensor of shape (N batch, N grid h, N grid w, N anchor , 2)\n",
    "    pred_box_wh   : tensor of shape (N batch, N grid h, N grid w, N anchor , 2)\n",
    "        \n",
    "    == output ==\n",
    "    \n",
    "    true_box_conf : tensor of shape (N batch, N grid h, N grid w, N anchor)\n",
    "    \n",
    "    true_box_conf value depends on the predicted values \n",
    "    true_box_conf = IOU_{true,pred} if objecte exist in this anchor else 0\n",
    "    '''\n",
    "    iou_scores        =  get_intersect_area(true_box_xy,true_box_wh,\n",
    "                                            pred_box_xy,pred_box_wh)\n",
    "    true_box_conf_IOU = iou_scores * true_box_conf\n",
    "    return true_box_conf_IOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true_box_conf_IOU.shape = (2, 13, 13, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment calc_IOU_pred_true_assigned\n",
    "\n",
    "true_box_conf_IOU = calc_IOU_pred_true_assigned(\n",
    "                            true_box_conf,\n",
    "                            true_box_xy, true_box_wh,\n",
    "                            pred_box_xy,  pred_box_wh)\n",
    "\n",
    "# print(\"*\"*30 + \"\\ninput\\n\" + \"*\"*30)    \n",
    "# print(\"true_box_conf = {}\".format(true_box_conf))\n",
    "# print(\"true_box_xy   = {}\".format(true_box_xy))\n",
    "# print(\"true_box_wh   = {}\".format(true_box_wh))\n",
    "# print(\"pred_box_xy   = {}\".format(pred_box_xy))\n",
    "# print(\"pred_box_wh   = {}\".format(pred_box_wh))\n",
    "# print(\"*\"*30 + \"\\nouput\\n\" + \"*\"*30) \n",
    "print(\"true_box_conf_IOU.shape = {}\".format(true_box_conf_IOU.shape))\n",
    "vec  = true_box_conf_IOU\n",
    "pick = vec!=0\n",
    "vec  = vec[pick]\n",
    "plt.hist(vec)\n",
    "plt.title(\"Histogram\\nN (%) nonzero true_box_conf_IOU = {} ({:5.2f}%)\".format(np.sum(pick),\n",
    "                                                             100*np.mean(pick)))\n",
    "plt.xlabel(\"nonzero true_box_conf_IOU\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Calculate $\\text{max}_{i',j'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_IOU_pred_true_best(pred_box_xy,pred_box_wh,true_boxes):   \n",
    "    '''\n",
    "    == input ==\n",
    "    pred_box_xy : tensor of shape (N batch, N grid h, N grid w, N anchor, 2)\n",
    "    pred_box_wh : tensor of shape (N batch, N grid h, N grid w, N anchor, 2)\n",
    "    true_boxes  : tensor of shape (N batch, N grid h, N grid w, N anchor, 2)\n",
    "    \n",
    "    == output == \n",
    "    \n",
    "    best_ious\n",
    "    \n",
    "    for each iframe,\n",
    "        best_ious[iframe,igridy,igridx,ianchor] contains\n",
    "        \n",
    "        the IOU of the object that is most likely included (or best fitted) \n",
    "        within the bounded box recorded in (grid_cell, anchor) pair\n",
    "        \n",
    "        NOTE: a same object may be contained in multiple (grid_cell, anchor) pair\n",
    "              from best_ious, you cannot tell how may actual objects are captured as the \"best\" object\n",
    "    '''\n",
    "    true_xy = true_boxes[..., 0:2]           # (N batch, 1, 1, 1, TRUE_BOX_BUFFER, 2)\n",
    "    true_wh = true_boxes[..., 2:4]           # (N batch, 1, 1, 1, TRUE_BOX_BUFFER, 2)\n",
    "    \n",
    "    pred_xy = tf.expand_dims(pred_box_xy, 4) # (N batch, N grid_h, N grid_w, N anchor, 1, 2)\n",
    "    pred_wh = tf.expand_dims(pred_box_wh, 4) # (N batch, N grid_h, N grid_w, N anchor, 1, 2)\n",
    "    \n",
    "    iou_scores  =  get_intersect_area(true_xy,\n",
    "                                      true_wh,\n",
    "                                      pred_xy,\n",
    "                                      pred_wh) # (N batch, N grid_h, N grid_w, N anchor, 50)   \n",
    "\n",
    "    best_ious = tf.reduce_max(iou_scores, axis=4) # (N batch, N grid_h, N grid_w, N anchor)\n",
    "    return best_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_ious.shape = (2, 13, 13, 4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Experiment calc_IOU_pred_true_best\n",
    "\n",
    "true_boxes = tf.constant(b_batch,dtype=\"float32\")\n",
    "best_ious = calc_IOU_pred_true_best(pred_box_xy,\n",
    "                                       pred_box_wh,\n",
    "                                       true_boxes)\n",
    "    \n",
    "# print(\"*\"*30 + \"\\ninput\\n\" + \"*\"*30)    \n",
    "# print(\"true_box_wh   = {}\".format(true_box_wh))\n",
    "# print(\"pred_box_xy   = {}\".format(pred_box_xy))\n",
    "# print(\"pred_box_wh   = {}\".format(pred_box_wh))\n",
    "# print(\"*\"*30 + \"\\nouput\\n\" + \"*\"*30) \n",
    "print(\"best_ious.shape = {}\".format(best_ious.shape))\n",
    "vec  = best_ious\n",
    "pick = vec!=0\n",
    "vec  = vec[pick]\n",
    "plt.hist(vec)\n",
    "plt.title(\"Histogram\\nN (%) nonzero best_ious = {} ({:5.2f}%)\".format(np.sum(pick),\n",
    "                                                             100*np.mean(pick)))\n",
    "plt.xlabel(\"nonzero best_ious\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conf_mask(best_ious, true_box_conf, true_box_conf_IOU,LAMBDA_NO_OBJECT, LAMBDA_OBJECT):    \n",
    "    '''\n",
    "    == input == \n",
    "    \n",
    "    best_ious           : tensor of shape (Nbatch, N grid h, N grid w, N anchor)\n",
    "    true_box_conf       : tensor of shape (Nbatch, N grid h, N grid w, N anchor)\n",
    "    true_box_conf_IOU   : tensor of shape (Nbatch, N grid h, N grid w, N anchor)\n",
    "    LAMBDA_NO_OBJECT    : 1.0\n",
    "    LAMBDA_OBJECT       : 5.0\n",
    "    \n",
    "    == output ==\n",
    "    conf_mask : tensor of shape (Nbatch, N grid h, N grid w, N anchor)\n",
    "    \n",
    "    conf_mask[iframe, igridy, igridx, ianchor] = 0\n",
    "               when there is no object assigned in (grid cell, anchor) pair and the region seems useless i.e. \n",
    "               y_true[iframe,igridx,igridy,4] = 0 \"and\" the predicted region has no object that has IoU > 0.6\n",
    "               \n",
    "    conf_mask[iframe, igridy, igridx, ianchor] =  NO_OBJECT_SCALE\n",
    "               when there is no object assigned in (grid cell, anchor) pair but region seems to include some object\n",
    "               y_true[iframe,igridx,igridy,4] = 0 \"and\" the predicted region has some object that has IoU > 0.6\n",
    "               \n",
    "    conf_mask[iframe, igridy, igridx, ianchor] =  OBJECT_SCALE\n",
    "              when there is an object in (grid cell, anchor) pair        \n",
    "    '''\n",
    "\n",
    "    conf_mask = tf.cast(best_ious < 0.6, tf.float32) * (1 - true_box_conf) * LAMBDA_NO_OBJECT\n",
    "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
    "    conf_mask = conf_mask + true_box_conf_IOU * LAMBDA_OBJECT\n",
    "    return conf_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "output\n",
      "******************************\n",
      "conf_mask shape = (2, 13, 13, 4)\n"
     ]
    }
   ],
   "source": [
    "# Experiment get_conf_mask\n",
    "\n",
    "conf_mask = get_conf_mask(best_ious, \n",
    "                             true_box_conf, \n",
    "                             true_box_conf_IOU,\n",
    "                             LAMBDA_NO_OBJECT, \n",
    "                             LAMBDA_OBJECT)\n",
    "# print(\"*\"*30 + \"\\ninput\\n\" + \"*\"*30)  \n",
    "# print(\"best_ious         = {}\".format(best_ious)) \n",
    "# print(\"true_box_conf     = {}\".format(true_box_conf))\n",
    "# print(\"true_box_conf_IOU = {}\".format(true_box_conf_IOU))\n",
    "# print(\"LAMBDA_NO_OBJECT  = {}\".format(LAMBDA_NO_OBJECT)) \n",
    "# print(\"LAMBDA_OBJECT     = {}\".format(LAMBDA_OBJECT))\n",
    "\n",
    "print(\"*\"*30 + \"\\noutput\\n\" + \"*\"*30)      \n",
    "print(\"conf_mask shape = {}\".format(conf_mask.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Calculate loss for the confidence lossci,j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_conf(conf_mask,true_box_conf_IOU, pred_box_conf):  \n",
    "    '''\n",
    "    == input ==\n",
    "    \n",
    "    conf_mask         : tensor of shape (Nbatch, N grid h, N grid w, N anchor)\n",
    "    true_box_conf_IOU : tensor of shape (Nbatch, N grid h, N grid w, N anchor)\n",
    "    pred_box_conf     : tensor of shape (Nbatch, N grid h, N grid w, N anchor)\n",
    "    '''\n",
    "    # the number of (grid cell, anchor) pair that has an assigned object or\n",
    "    # that has no assigned object but some objects may be in bounding box.\n",
    "    # N conf\n",
    "    nb_conf_box  = tf.reduce_sum(tf.cast(conf_mask  > 0.0, tf.float32))\n",
    "    loss_conf    = tf.reduce_sum(tf.square(true_box_conf_IOU-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
    "    return loss_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "output\n",
      "******************************\n",
      "loss_conf = 0.1246\n"
     ]
    }
   ],
   "source": [
    "# Experiment calc_loss_conf\n",
    "\n",
    "# print(\"*\"*30 + \"\\ninput\\n\" + \"*\"*30)  \n",
    "# print(\"conf_mask         = {}\".format(conf_mask)) \n",
    "# print(\"true_box_conf_IOU = {}\".format(true_box_conf_IOU))\n",
    "# print(\"pred_box_conf     = {}\".format(pred_box_conf))\n",
    "\n",
    "loss_conf = calc_loss_conf(conf_mask,true_box_conf_IOU, pred_box_conf)\n",
    "    \n",
    "print(\"*\"*30 + \"\\noutput\\n\" + \"*\"*30)      \n",
    "print(\"loss_conf = {:5.4f}\".format(loss_conf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom_loss(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    '''\n",
    "    y_true : (N batch, N grid h, N grid w, N anchor, 4 + 1 + N classes)\n",
    "    y_true[irow, i_gridh, i_gridw, i_anchor, :4] = center_x, center_y, w, h\n",
    "    \n",
    "        center_x : The x coordinate center of the bounding box.\n",
    "                   Rescaled to range between 0 and N gird  w (e.g., ranging between [0,13)\n",
    "        center_y : The y coordinate center of the bounding box.\n",
    "                   Rescaled to range between 0 and N gird  h (e.g., ranging between [0,13)\n",
    "        w        : The width of the bounding box.\n",
    "                   Rescaled to range between 0 and N gird  w (e.g., ranging between [0,13)\n",
    "        h        : The height of the bounding box.\n",
    "                   Rescaled to range between 0 and N gird  h (e.g., ranging between [0,13)\n",
    "                   \n",
    "    y_true[irow, i_gridh, i_gridw, i_anchor, 4] = ground truth confidence\n",
    "        \n",
    "        ground truth confidence is 1 if object exists in this (anchor box, gird cell) pair\n",
    "    \n",
    "    y_true[irow, i_gridh, i_gridw, i_anchor, 5 + iclass] = 1 if the object is in category  else 0\n",
    "        \n",
    "    '''\n",
    "    total_recall = tf.Variable(0.)\n",
    "    \n",
    "    # Step 1: Adjust prediction output\n",
    "    cell_grid   = get_cell_grid(GRID_W,GRID_H,BATCH_SIZE,BOX)\n",
    "    pred_box_xy, pred_box_wh, pred_box_conf, pred_box_class = adjust_scale_prediction(y_pred,cell_grid,ANCHORS)\n",
    "    # Step 2: Extract ground truth output\n",
    "    true_box_xy, true_box_wh, true_box_conf, true_box_class = extract_ground_truth(y_true)\n",
    "    # Step 3: Calculate loss for the bounding box parameters\n",
    "    loss_xywh, coord_mask = calc_loss_xywh(true_box_conf,LAMBDA_COORD,\n",
    "                                           true_box_xy, pred_box_xy,true_box_wh,pred_box_wh)\n",
    "    # Step 4: Calculate loss for the class probabilities\n",
    "    loss_class  = calc_loss_class(true_box_conf,LAMBDA_CLASS,\n",
    "                                   true_box_class,pred_box_class)\n",
    "    # Step 5: For each (grid cell, anchor) pair, \n",
    "    #         calculate the IoU between predicted and ground truth bounding box\n",
    "    true_box_conf_IOU = calc_IOU_pred_true_assigned(true_box_conf,\n",
    "                                                    true_box_xy, true_box_wh,\n",
    "                                                    pred_box_xy, pred_box_wh)\n",
    "    # Step 6: For each predicted bounded box from (grid cell, anchor box), \n",
    "    #         calculate the best IOU, regardless of the ground truth anchor box that each object gets assigned.\n",
    "    best_ious = calc_IOU_pred_true_best(pred_box_xy,pred_box_wh,true_boxes)\n",
    "    # Step 7: For each grid cell, calculate the L_{i,j}^{noobj}\n",
    "    conf_mask = get_conf_mask(best_ious, true_box_conf, true_box_conf_IOU,LAMBDA_NO_OBJECT, LAMBDA_OBJECT)\n",
    "    # Step 8: Calculate loss for the confidence\n",
    "    loss_conf = calc_loss_conf(conf_mask,true_box_conf_IOU, pred_box_conf)\n",
    "\n",
    "    \n",
    "    loss = loss_xywh + loss_conf + loss_class\n",
    "    \n",
    "\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64 float64\n",
      "loss tf.Tensor(5.663559, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Experiment custom_loss\n",
    "\n",
    "print(y_batch.dtype, y_pred.dtype)\n",
    "true_boxes = tf.Variable(np.zeros_like(b_batch), dtype=\"float32\")\n",
    "loss = custom_loss(y_batch.astype('float32'), y_pred.astype('float32')) \n",
    "print('loss', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolo-TF2-VKR",
   "language": "python",
   "name": "yolo-tf2-vkr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
